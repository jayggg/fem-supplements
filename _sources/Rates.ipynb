{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergent quantities, such as errors in numerical approximations of functions,  often decrease to zero with respect to some approximation space parameters. Being able to compute the *rate of convergence* of such quantities is important in computational experimentation. \n",
    "\n",
    "In a [prior activity](BestApprox.ipynb), we computed the $L_2$ best approximation of some given function $u$ from a finite element space $S_h$. Given a function $\\newcommand{\\om}{\\varOmega}$ $u \\in L_2(\\om)$, its **Best Approximation Error** from a finite element subspace $S_h$ of $L_2(\\om)$ is\n",
    "\n",
    "$$\n",
    "\\text{BAE}(u) = \\min_{v \\in S_h} \\| u -v \\|_{L_2(\\om)}.\n",
    "$$\n",
    "\n",
    "```{index} best approximation error (BAE)\n",
    "```\n",
    "```{index} best approximation\n",
    "```\n",
    "\n",
    "In this notebook, we are interested in quantifying how this BAE becomes small as the mesh size $h$ is decreased  and as the polynomial degree $p$ increases. This is done by computing rates of observed convergence. To fix a finite element space, we set $S_h$ to $W_{hp}$ throughout: recall that it is the \"DG space\" $ W_{hp} =  \\{ w: w|_K$ is a polynomial of degree $\\le p$ on each mesh element $K\\}.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "```{index} projection; L2-orthogonal\n",
    "```\n",
    "\n",
    "We saw that the best approximations can be computed by minimizing a nonlinear functional in  a [previous notebook](BestApprox.ipynb). In exercises, we have also seen that the mapping from $u$ to its $L_2$-best approximation from a finite element space is equal to the *$L_2$-orthogonal projection* onto the  finite element space. Let $\\newcommand{\\om}{\\varOmega}$ $Q_{hp}: L_2(\\om) \\to W_{hp}$ denote the $L_2$-orthogonal projection onto $W_{hp}$. The norm and the inner product in $L_2(\\om)$ are, respectively, defined by  \n",
    "\n",
    "$$\n",
    "(u,v)_{L_2(\\om)} = \\int_{\\om} u(x) v(x)\\, dx, \\qquad \\| u \\|_{L_2(\\om)} ={(u,u)^{1/2}_{L_2(\\om)}}.\n",
    "$$\n",
    "\n",
    "We will use the following two properties from exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Property 1.*    The $L_2$-projection $Q_{hp} u$ is the best approximation to a $\\newcommand{\\om}{\\varOmega}$ $u \\in L_2(\\om)$ from the subspace $M$,   namely, \n",
    "\n",
    "$$\\tag{1}\n",
    "\\| u - Q_{hp} u \\|_{L_2(\\om)} = \\min_{w \\in W_{hp}} \\| u - w \\|_{L_2(\\om)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Property 2.*  $Q_{hp} u$ satisfies $\\newcommand{\\om}{\\varOmega}$\n",
    "\n",
    "$$\\tag{2}\n",
    "(Q_{hp} u , w)_{L_2(\\om)} = (u, w)_{L_2(\\om)}\\quad \\text{ for all }  w  \\in W_{hp}\n",
    "$$\n",
    "\n",
    "and  any $u \\in L_2(\\om)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right hand side of equation (1) equals the BAE$(u)$ of $u$ from $W_{hp}$. Thus Property 1\n",
    "implies that we can compute BAE$(u)$ if we can compute $Q_{hp} u$ and the norm of the difference  $\\| u - Q_{hp} u \\|_{L_2(\\om)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the $L_2$-orthogonal projection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property 2 gives us a way to compute the $L_2$-orthogonal projection by *solving  a linear system*, as we shall see below. Property 1 then tells us that we have in fact computed the best approximation. Therefore this gives us an alternate *linear* method, (at least superficially) distinct from the *nonlinear* method of a [prior notebook](BestApprox.ipynb), to compute best approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namely, given a $u \\in L_2(\\om)$, by (2), its best approximation from $W_{hp}$ is the function $q = Q_{hp} u$ in $W_{hp}$ satisfying \n",
    "\n",
    "$$\\newcommand{\\om}{\\varOmega}\n",
    "(q , w)_{L_2(\\om)} = (u, w)_{L_2(\\om)}\\quad \\text{ for all }  w  \\in W_{hp}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encounter many more equations of this type in this course, so it pays to delve into a bit more detail. All such equations will be of the form\n",
    "\n",
    "$$\n",
    "\\text{ find } q \\in W: \\qquad a(q, w) = b(w), \\qquad \\text{ for all } w \\in W\n",
    "$$\n",
    "\n",
    "and they are often called **equations of forms** or **variational equations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the projection computation, $a(\\cdot,\\cdot)$ is the just the inner product $(\\cdot, \\cdot)_{L_2(\\om)}$, while $b(\\cdot)$ equals $(u, \\cdot)_{L_2(\\om)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us  get acquainted with some terminology related to equations like $a(q, w) = b(w)$  right away:\n",
    "\n",
    "- The set of functions from which the solution $q$  is to be found is called the set of **trial functions**.\n",
    "\n",
    "```{index} trial function\n",
    "```\n",
    "\n",
    "- The set of functions $m$ for which the equation must hold is called the set of  **test functions**. (In the projection example, the spaces of trial and test functions are the same, but they need not always be so.)\n",
    "\n",
    "```{index} test function\n",
    "```\n",
    "\n",
    "- The right hand side is called a **linear form** since it is linear in the test function $w$. (In the projection example, it contains the problem data involving the given function $u$.)\n",
    "\n",
    "```{index} linear form\n",
    "```\n",
    "\n",
    "- The left hand side is called a **bilinear form** (since it is linear in both the trial function $q$ and the test function $w$).\n",
    "\n",
    "```{index} bilinear form\n",
    "```\n",
    "\n",
    "*You will see these names being used for the corresponding ngsolve objects below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngsolve as ng\n",
    "from netgen.occ import unit_square\n",
    "from ngsolve import dx, x, y, sin, BilinearForm, LinearForm, Mesh\n",
    "\n",
    "def ProjectL2(u, W):\n",
    "    \"\"\" Input: u as a CF.\n",
    "        Output: L_2-projection Q u as a GF. \"\"\"\n",
    "    Qu = ng.GridFunction(W)\n",
    "    q = W.TrialFunction()  # Alternate one-liner:\n",
    "    w = W.TestFunction()   #   q, w = W.TnT()\n",
    "    A = BilinearForm(q*w*dx)\n",
    "    B = LinearForm(u*w*dx)\n",
    "    \n",
    "    with ng.TaskManager():\n",
    "        A.Assemble()\n",
    "        B.Assemble()\n",
    "        Qu.vec.data = A.mat.Inverse(inverse='sparsecholesky') * B.vec\n",
    "    return Qu    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before explaining the code, let us visualize a smooth function and its projection into the space of piecewise constant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve.webgui import Draw\n",
    "\n",
    "h = 0.2\n",
    "mesh = ng.Mesh(unit_square.GenerateMesh(maxh=h))\n",
    "p = 0    # the piecewise constant DG space\n",
    "Whp = ng.L2(mesh, order=p)\n",
    "\n",
    "u = sin(5 * x * y)\n",
    "Qu = ProjectL2(u, Whp)\n",
    "\n",
    "Draw(u, mesh); Draw(Qu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to a matrix problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, notice the `Assemble` and `Inverse` methods. The first converts equations of forms \n",
    "\n",
    "\n",
    "$$\\tag{3}\n",
    "\\text{ find } q\\in W: \\qquad a(q, w) = b(w), \\qquad \\text{ for all } w \\in W\n",
    "$$\n",
    "\n",
    "\n",
    "in a generic space $W$, represented as `W` in the computations,  into a *matrix system*, using a basis for `W`, as explained next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\Theta_1, \\ldots, \\Theta_N$ form a set of global shape functions of the finite element space `W`.  (We have talked about global shape functions both in [1D](OneDim.ipynb) and in [2D](BestApprox.ipynb) for several finite element spaces in prior notebooks.)  Then expanding the unknown function $q \\in W$ in the basis of global shape functions, \n",
    "\n",
    "$$\\tag{4}\n",
    "q = \\sum_{j=1}^N X_j \\Theta_j,\n",
    "$$\n",
    "\n",
    "we find that, with $w=\\Theta_i$, the equation (3) imply \n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^N a(\\Theta_j, \\Theta_i)\\, X_j = b(\\Theta_i)\n",
    "$$\n",
    "\n",
    "for every $i=1, \\ldots, N$. Hence, defining a matrix $A \\in \\mathbb{R}^{N \\times N}$ and a vector $B \\in \\mathbb{R}^N$ by \n",
    "\n",
    "$$\n",
    "A_{ij} = a(\\Theta_j, \\Theta_i), \\qquad B_i = b(\\Theta_i),\n",
    "$$\n",
    "\n",
    "we have converted the problem of finding $q \\in W$ into a problem of finding a vector $X \\in \\mathbb{R}^N$ satisfying \n",
    "\n",
    "$$\n",
    "A X = B.\n",
    "$$\n",
    "\n",
    "This is the process that happens behind the scenes when the `Assemble` method is called. In the code for the function `ProjectL2`,  the object `a.mat` contains the matrix $A$ (in a sparse format) and `b.vec` contains the vector $B$ after the `Assemble` methods are completed. Matrices $A$ generated in this way from a bilinear form $a(\\cdot, \\cdot)$ are called **stiffness matrices** of the form $a$.\n",
    "\n",
    "```{index} stiffness matrix\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the vector $X$, \n",
    "\n",
    "$$\n",
    "X = A^{-1} B\n",
    "$$\n",
    "\n",
    "we use the line in the code where `A.mat.Inverse(...)` is multiplied by `B.vec`.  Knowing the vector $X$ is the same as knowing everything about the function $q$ due to (4).\n",
    "\n",
    "The `inverse=sparsecholesky` keyword within the `Inverse` method calls an ngsolve implementation of a version of Cholesky factorization for sparse matrices. It only works for symmetric/Hermitian and positive definite matrices. (Is the matrix for the projection problem such a matrix?) For more general sparse matrices, it is advisable to use a professional sparse matrix solver such as umfpack, pardiso, or mumps. A standard installation of ngsolve currently contains [umfpack](https://people.engr.tamu.edu/davis/suitesparse.html), is accessible by `inverse=umfpack`, and is the default option for the `inverse` keyword.\n",
    "\n",
    "```{index} sparse matrix solver\n",
    "```\n",
    "\n",
    "```{index} umfpack\n",
    "```\n",
    "\n",
    "```{index} sparsecholesky\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successive mesh refinements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to study the best approximation error over a collection of meshes whose *element shapes* do not vary too much over the collection, even if their *element sizes* might vary considerably. A sequence of increasingly finer meshes where the element angles never change from those of the coarsest mesh is obtained by a simple *uniform refinement*. This is illustrated in the sequence of mesh images below.\n",
    "\n",
    "```{index} refinement; uniform\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mesh = Mesh(unit_square.GenerateMesh(maxh=0.3))\n",
    "Draw(mesh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.ngmesh.Refine(); Draw(mesh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.ngmesh.Refine(); Draw(mesh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this process of uniform refinement, when started on any coarse mesh, does not generate any new angles within the new elements, i.e., each triangular element of this infinite sequence of meshes is similar (in the geometric sense) to one of the triangles in the initial coarse mesh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections on uniform refinements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the $L_2$ projection on a sequence of uniform refinements, starting from a very coarse mesh, using the function below. We then compute the $L_2$-norm of the error $\\| u - Q_{hp} u\\|_{L_2(\\om)}$ using the `Integrate` method provided by ngsolve.\n",
    "\n",
    "```{index} integrate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import Integrate, sqrt\n",
    "import numpy as np\n",
    "\n",
    "def ProjectOnSuccessiveRefinements(u, p=0, hcoarse=1, nrefinements=8):                                   \n",
    "    \"\"\"Project to f.e. spaces on a sequence of uniformly refined meshes.\"\"\"\n",
    "    \n",
    "    Qus = []; errors = []; meshes = []; ndofs = []\n",
    "    mesh = ng.Mesh(unit_square.GenerateMesh(maxh=hcoarse))\n",
    "    \n",
    "    for ref in range(nrefinements): \n",
    "        W = ng.L2(mesh, order=p)\n",
    "        Qu = ProjectL2(u, W)         \n",
    "        sqrerr = (Qu - u)**2\n",
    "      \n",
    "        Qus.append(Qu) \n",
    "        errors.append(sqrt(Integrate(sqrerr, mesh)))\n",
    "        meshes.append(ng.Mesh(mesh.ngmesh.Copy()))\n",
    "        ndofs.append(W.ndof) \n",
    "      \n",
    "        mesh.ngmesh.Refine()\n",
    "\n",
    "    return Qus, np.array(errors), ndofs, meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  select an infinitely smooth function $u$ for the initial experiments (and you can experiment with less regular functions in exercises). Additionally, let us ensure that the smooth $u$ has  some mild oscillations  (as otherwise the approximation errors reduce very rapidly and become too close to machine precision and rate computation becomes unreliable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = sin(5*x*y)\n",
    "Qus, es, ns, _ = ProjectOnSuccessiveRefinements(u, p=0)\n",
    "errors = {0: es}; ndofs = {0: ns}\n",
    "es  # display the sequence of error norms just computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this sequence of error norms are decreasing, approximately halving at each step. This immediately gives us an indication of the expected rate of convergence of the error: indeed, any quantity that decreases to zero like $O(h)$ will halve each time the mesh size $h$ is halved. (In contrast, a quantity that decreases like $O(h^2)$ will quarter each time $h$ is halved.)\n",
    "Anticipating more general error sequences in other cases, let us get organized for systematic rate computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate rate of convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate at what rate the errors on successive refinements (stored in the above list `es`) go to zero.  Let $e_i$ denote the $i$th element of the list.\n",
    "\n",
    "\n",
    "Specifically, what we would like to determine is the number $r$, the **rate of convergence**, such that the errors are bounded by $O(h^r).$\n",
    "If the sequence $e_i$ goes to zero like $O(h_i^r)$, then  since the refinement pattern dictates\n",
    "\n",
    "$$\n",
    "h_i = \\frac{h_0}{2^i}, \n",
    "$$ \n",
    "\n",
    "we should see $e_{i+1} / e_i  \\sim O(2^{-r})$. Hence to estimate $r$, we compute \n",
    "\n",
    "$$\n",
    "\\log_2 \\frac{e_{i+1}}{ e_i}.\n",
    "$$\n",
    "\n",
    "These logarithms are computed and tabulated together with the error numbers using the formatting function below. For formatting, we use a small external library called `prettytable`. \n",
    "\n",
    "```{index} convergence rates\n",
    "```\n",
    "```{index} pretty table\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def TabulateRate(name, dat, h0=1):\n",
    "    \"\"\"Inputs: \n",
    "        name = Name for second (error norm) column, \n",
    "        dat = list of error data on successive refinements\n",
    "        log2h0inv = log2(1/h0), where h0 is coarsest meshsize.\n",
    "    \"\"\"\n",
    "    col = ['h', name, 'rate']\n",
    "    t = PrettyTable()\n",
    "    h0col = ['%g'%h0]\n",
    "    t.add_column(col[0], h0col + [h0col[0] + '/' + str(2**i) \n",
    "                                  for i in range(1, len(dat))])\n",
    "    t.add_column(col[1], ['%.12f'%e for e in dat])\n",
    "    t.add_column(col[2], ['*'] + \\\n",
    "                 ['%1.2f' % r \n",
    "                  for r in np.log(dat[:-1]/dat[1:])/np.log(2)])\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will apply this function to tabulate rates of convergence for each $p$ using a series of uniformly refined meshes. Both the error norms and the rate computed using the log-technique are tabulated for each degree considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p=0$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TabulateRate('L2norm(Pu-u)', es)  # compute rate & tabulate previous p=0 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p=1$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayL2BAE(u, p=1):\n",
    "    _, es, ns, _ = ProjectOnSuccessiveRefinements(u, p=p)\n",
    "    errors[p] = es; ndofs[p] = ns  # store for later use\n",
    "    TabulateRate('L2norm(Pu-u)', es)\n",
    "\n",
    "DisplayL2BAE(u, p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p=2$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayL2BAE(u, p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p=3$ case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayL2BAE(u, p=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p=4$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayL2BAE(u, p=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These observations clearly suggest that the rate of convergence of this BAE from $W_{hp}$ appears to be $p+1$ when polynomial degree $p$ is used. \n",
    "\n",
    "Indeed, we will prove in a later lecture that there is a constant $C_u$ independent of the mesh size  $h$, but dependent on the smooth function $u$, such that \n",
    "\n",
    "$$\n",
    "\\inf_{ w \\in W_{hp}} \\| u - w \\|_{L_2(\\om)} \\le C_u h^{p+1}.\n",
    "$$\n",
    "\n",
    "Rigorous approximation estimates like these form a basic pillar of finite element theory.\n",
    "\n",
    "**Questions for discussion:**\n",
    "\n",
    "- Do you have any intuition on why BAE decreases when $h\\to 0$?\n",
    "- Why does the rate of convergence increase with increasing $p$?\n",
    "- What happens when you try a less smooth function $u$?\n",
    "\n",
    "How fast the best approximation error decreases as meshes get finer is an issue that is intimately connected to the smoothness or regularity of the function being approximated. This is an extensively studied topic in the field of approximation theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs. degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the above computations, we have also stored $\\dim(W_{hp})$, or the **number of degrees of freedom** in `ndofs`. This is useful when trying to gauge how much bang for the buck we obtain when using various meshes and various orders $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for p in range(5):\n",
    "    plt.loglog(ndofs[p], errors[p], '.-', label='p=%d'%p)\n",
    "\n",
    "plt.xlabel('Degrees of Freedom')\n",
    "plt.ylabel('$L_2$ best approximation error'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from these results that for the smooth `u` under consideration,  the accuracy of its best approximation, measured *per* degree of freedom increases (quite dramatically) when using higher $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have seen\n",
    "\n",
    "- trial and test functions in variational equations,\n",
    "- orthogonal projections and distance minimizers,\n",
    "- assembly of stiffness matrix of a bilinear form,\n",
    "- sparse matrix inversion facility,\n",
    "- how to integrate expressions over a mesh,\n",
    "- how to perform uniform refinement of an unstructured mesh,\n",
    "- how to compute and tabulate convergence rates,\n",
    "- how best approximation errors decrease with $h$ and $p$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": ""
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
